{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('venv': venv)",
   "metadata": {
    "interpreter": {
     "hash": "50c302c728f6287441bb362f6f3cf806e9eb81794a9371ef18d697ceec0601a8"
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import random\n",
    "import tensorflow as tf\n",
    "\n",
    "import loader as ld\n",
    "import unet\n",
    "import reporter as rp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "usage: python main.py\nImage segmentation using U-Net: error: unrecognized arguments: --ip=127.0.0.1 --stdin=9244 --control=9242 --hb=9241 --Session.signature_scheme=\"hmac-sha256\" --Session.key=b\"28913674-c763-474c-b12a-a0bfabee46a1\" --shell=9243 --transport=\"tcp\" --iopub=9245 --f=/var/folders/mj/y0s2tdjd66v4bvdgf0_q0xt40000gn/T/tmp-449281ZuJ4ts8lURF.json\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "SystemExit",
     "evalue": "2",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "def load_dataset(train_rate):\n",
    "    loader = ld.Loader(dir_original=\"data_set/VOCdevkit/VOC2012/JPEGImages\",\n",
    "                       dir_segmented=\"data_set/VOCdevkit/VOC2012/SegmentationClass\")\n",
    "    return loader.load_train_test(train_rate=train_rate, shuffle=False)\n",
    "\n",
    "\n",
    "def train(parser):\n",
    "    # 訓練とテストデータを読み込みます\n",
    "    # Load train and test datas\n",
    "    train, test = load_dataset(train_rate=parser.trainrate)\n",
    "    valid = train.perm(0, 30)\n",
    "    test = test.perm(0, 150)\n",
    "\n",
    "    # 結果保存用のインスタンスを作成します\n",
    "    # Create Reporter Object\n",
    "    reporter = rp.Reporter(parser=parser)\n",
    "    accuracy_fig = reporter.create_figure(\"Accuracy\", (\"epoch\", \"accuracy\"), [\"train\", \"test\"])\n",
    "    loss_fig = reporter.create_figure(\"Loss\", (\"epoch\", \"loss\"), [\"train\", \"test\"])\n",
    "\n",
    "    # GPUを使用するか\n",
    "    # Whether or not using a GPU\n",
    "    gpu = parser.gpu\n",
    "\n",
    "    # モデルの生成\n",
    "    # Create a model\n",
    "    model_unet = model.UNet(l2_reg=parser.l2reg).model\n",
    "\n",
    "    # 誤差関数とオプティマイザの設定をします\n",
    "    # Set a loss function and an optimizer\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=model_unet.teacher,\n",
    "                                                                           logits=model_unet.outputs))\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    with tf.control_dependencies(update_ops):\n",
    "        train_step = tf.train.AdamOptimizer(0.001).minimize(cross_entropy)\n",
    "\n",
    "    # 精度の算出をします\n",
    "    # Calculate accuracy\n",
    "    correct_prediction = tf.equal(tf.argmax(model_unet.outputs, 3), tf.argmax(model_unet.teacher, 3))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    # セッションの初期化をします\n",
    "    # Initialize session\n",
    "    gpu_config = tf.ConfigProto(gpu_options=tf.GPUOptions(per_process_gpu_memory_fraction=0.7), device_count={'GPU': 1},\n",
    "                                log_device_placement=False, allow_soft_placement=True)\n",
    "    sess = tf.InteractiveSession(config=gpu_config) if gpu else tf.InteractiveSession()\n",
    "    tf.global_variables_initializer().run()\n",
    "\n",
    "    # モデルの訓練\n",
    "    # Train the model\n",
    "    epochs = parser.epoch\n",
    "    batch_size = parser.batchsize\n",
    "    is_augment = parser.augmentation\n",
    "    train_dict = {model_unet.inputs: valid.images_original, model_unet.teacher: valid.images_segmented,\n",
    "                  model_unet.is_training: False}\n",
    "    test_dict = {model_unet.inputs: test.images_original, model_unet.teacher: test.images_segmented,\n",
    "                 model_unet.is_training: False}\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        for batch in train(batch_size=batch_size, augment=is_augment):\n",
    "            # バッチデータの展開\n",
    "            inputs = batch.images_original\n",
    "            teacher = batch.images_segmented\n",
    "            # Training\n",
    "            sess.run(train_step, feed_dict={model_unet.inputs: inputs, model_unet.teacher: teacher,\n",
    "                                            model_unet.is_training: True})\n",
    "\n",
    "        # 評価\n",
    "        # Evaluation\n",
    "        if epoch % 1 == 0:\n",
    "            loss_train = sess.run(cross_entropy, feed_dict=train_dict)\n",
    "            loss_test = sess.run(cross_entropy, feed_dict=test_dict)\n",
    "            accuracy_train = sess.run(accuracy, feed_dict=train_dict)\n",
    "            accuracy_test = sess.run(accuracy, feed_dict=test_dict)\n",
    "            print(\"Epoch:\", epoch)\n",
    "            print(\"[Train] Loss:\", loss_train, \" Accuracy:\", accuracy_train)\n",
    "            print(\"[Test]  Loss:\", loss_test, \"Accuracy:\", accuracy_test)\n",
    "            accuracy_fig.add([accuracy_train, accuracy_test], is_update=True)\n",
    "            loss_fig.add([loss_train, loss_test], is_update=True)\n",
    "            if epoch % 3 == 0:\n",
    "                idx_train = random.randrange(10)\n",
    "                idx_test = random.randrange(100)\n",
    "                outputs_train = sess.run(model_unet.outputs,\n",
    "                                         feed_dict={model_unet.inputs: [train.images_original[idx_train]],\n",
    "                                                    model_unet.is_training: False})\n",
    "                outputs_test = sess.run(model_unet.outputs,\n",
    "                                        feed_dict={model_unet.inputs: [test.images_original[idx_test]],\n",
    "                                                   model_unet.is_training: False})\n",
    "                train_set = [train.images_original[idx_train], outputs_train[0], train.images_segmented[idx_train]]\n",
    "                test_set = [test.images_original[idx_test], outputs_test[0], test.images_segmented[idx_test]]\n",
    "                reporter.save_image_from_ndarray(train_set, test_set, train.palette, epoch,\n",
    "                                                 index_void=len(ld.DataSet.CATEGORY)-1)\n",
    "\n",
    "    # 訓練済みモデルの評価\n",
    "    # Test the trained model\n",
    "    loss_test = sess.run(cross_entropy, feed_dict=test_dict)\n",
    "    accuracy_test = sess.run(accuracy, feed_dict=test_dict)\n",
    "    print(\"Result\")\n",
    "    print(\"[Test]  Loss:\", loss_test, \"Accuracy:\", accuracy_test)\n",
    "\n",
    "    sess.close()\n",
    "\n",
    "\n",
    "def get_parser():\n",
    "    parser = argparse.ArgumentParser(\n",
    "        prog='Image segmentation using U-Net',\n",
    "        usage='python main.py',\n",
    "        description='This module demonstrates image segmentation using U-Net.',\n",
    "        add_help=True\n",
    "    )\n",
    "\n",
    "    parser.add_argument('-g', '--gpu', action='store_true', help='Using GPUs')\n",
    "    parser.add_argument('-e', '--epoch', type=int, default=250, help='Number of epochs')\n",
    "    parser.add_argument('-b', '--batchsize', type=int, default=32, help='Batch size')\n",
    "    parser.add_argument('-t', '--trainrate', type=float, default=0.85, help='Training rate')\n",
    "    parser.add_argument('-a', '--augmentation', action='store_true', help='Number of epochs')\n",
    "    parser.add_argument('-r', '--l2reg', type=float, default=0.0001, help='L2 regularization')\n",
    "\n",
    "    return parser\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = get_parser().parse_args()\n",
    "    train(parser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}